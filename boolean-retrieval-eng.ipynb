{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10635435,"sourceType":"datasetVersion","datasetId":6584867}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\nroot_dir = \"/english/en.doc.2010/English-Data\"\ndest_dir = \"/english/en.doc.2010/English-Data/All_Files\"\n\nos.makedirs(dest_dir, exist_ok=True)\n\nfor subdir, _, files in os.walk(root_dir, topdown=False):\n    for file in files:\n        if file.endswith(\".utf8\"):\n            file_path = os.path.join(subdir, file)\n            shutil.move(file_path, os.path.join(dest_dir, file))\n\nfor subdir, _, _ in os.walk(root_dir, topdown=False):\n    if not os.listdir(subdir):\n        os.rmdir(subdir)\n\nprint(\"All .utf8 files moved successfully, and empty folders deleted!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\n\ninput_dir = \"/home/achal/Downloads/english/documents\"\ntags_pattern = r'</?(DOC|TEXT)>|<DOCNO>.*?</DOCNO>'\ndatetime_pattern = r'\\[\\s*[A-Za-z]+,\\s+[A-Za-z]+\\s+\\d{1,2},\\s+\\d{4}\\s+\\d{2}:\\d{2}:\\d{2}\\s+[apAP][mM]\\s*\\]'\npunctuation_pattern = r'[ред!?.,:;\\'\\-\"()\\[\\]{}рее]+'\n\nfor filename in os.listdir(input_dir):\n    if filename.endswith(\".utf8\"):\n        file_path = os.path.join(input_dir, filename)\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            content = re.sub(tags_pattern, '', content)\n            content = re.sub(datetime_pattern, '', content)\n            content = re.sub(punctuation_pattern, '', content)\n            content = content.lower()\n            content = ' '.join(content.split())\n        with open(file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n\nprint(\"All documents have been cleaned in-place!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Converting to Pandas Dataframe","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ninput_dir = \"/home/achal/Downloads/english/documents\"\ndata = []\n\nfor filename in os.listdir(input_dir):\n    if filename.endswith(\".utf8\"):\n        file_path = os.path.join(input_dir, filename)\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        data.append({\"filename\": filename, \"content\": content})\n\ndf = pd.DataFrame(data)\nprint(df.head())\ndf.to_csv(\"documents.csv\", index=False, encoding=\"utf-8\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = '/kaggle/input/eng-documents/eng_documents.csv'\ndf = pd.read_csv(file_path)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:38:10.026007Z","iopub.execute_input":"2025-02-01T19:38:10.026318Z","iopub.status.idle":"2025-02-01T19:38:13.260841Z","shell.execute_reply.started":"2025-02-01T19:38:10.026291Z","shell.execute_reply":"2025-02-01T19:38:13.260107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Cleaning","metadata":{}},{"cell_type":"code","source":"redundant_title = \"the telegraph calcutta \"\n\ndef clean_title(text):\n    if not isinstance(text, str):\n        return text\n    text = text.strip()\n    if text.lower().startswith(redundant_title):\n        return text[len(redundant_title):].strip()\n    return text\n\ndf[\"content\"] = df[\"content\"].astype(str).apply(clean_title)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:38:22.225588Z","iopub.execute_input":"2025-02-01T19:38:22.225869Z","iopub.status.idle":"2025-02-01T19:38:22.704714Z","shell.execute_reply.started":"2025-02-01T19:38:22.225848Z","shell.execute_reply":"2025-02-01T19:38:22.703964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stopword Removal","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")\nstop_words = set(stopwords.words(\"english\"))\n\ndef remove_stopwords(text):\n    if not isinstance(text, str):\n        return text\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return \" \".join(filtered_words)\n\ndf[\"content\"] = df[\"content\"].apply(remove_stopwords)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:38:26.341181Z","iopub.execute_input":"2025-02-01T19:38:26.341467Z","iopub.status.idle":"2025-02-01T19:38:36.047005Z","shell.execute_reply.started":"2025-02-01T19:38:26.341447Z","shell.execute_reply":"2025-02-01T19:38:36.046081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stemming","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\ndef apply_stemming(text):\n    if not isinstance(text, str):\n        return text\n    words = text.split()\n    stemmed_words = [stemmer.stem(word) for word in words]\n    return \" \".join(stemmed_words)\n\ndf[\"content\"] = df[\"content\"].apply(apply_stemming)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:38:36.124753Z","iopub.execute_input":"2025-02-01T19:38:36.125001Z","iopub.status.idle":"2025-02-01T19:46:57.261356Z","shell.execute_reply.started":"2025-02-01T19:38:36.124980Z","shell.execute_reply":"2025-02-01T19:46:57.260376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calculating TF (Term-Frequency) and DF (Document-Frequency)","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport pandas as pd\n\nindex = defaultdict(lambda: defaultdict(int)) # inverted indexing\n\nfor _, row in df.iterrows(): # Building the posting list\n    filename = row[\"filename\"]\n    words = row[\"content\"].split()\n    for word in words:\n        index[word][filename] += 1  # Increase term frequency (TF) for the word in this document\n\nindex_data = []\nfor term, postings in index.items():\n    doc_frequency = len(postings)  # Number of documents containing this term (DF)\n    for doc, tf in postings.items():\n        index_data.append((term, doc, tf, doc_frequency))\n\nindex_df = pd.DataFrame(index_data, columns=[\"Term\", \"Document\", \"Term Frequency\", \"Document Frequency\"])\nindex_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:40.271206Z","iopub.execute_input":"2025-02-01T18:59:40.271510Z","iopub.status.idle":"2025-02-01T19:00:27.315515Z","shell.execute_reply.started":"2025-02-01T18:59:40.271489Z","shell.execute_reply":"2025-02-01T19:00:27.314725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:00:27.316322Z","iopub.execute_input":"2025-02-01T19:00:27.316662Z","iopub.status.idle":"2025-02-01T19:00:28.251416Z","shell.execute_reply.started":"2025-02-01T19:00:27.316639Z","shell.execute_reply":"2025-02-01T19:00:28.250374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Printing Posting List for a term","metadata":{}},{"cell_type":"code","source":"def print_posting_list(term):\n    if term in index:\n        print(f\"Posting List for '{term}':\")\n        for doc, tf in index[term].items():\n            df = index[term][doc]\n            print(f\"Document: {doc}, Term Frequency: {tf}, Document Frequency: {df}\")\n    else:\n        print(f\"Term '{term}' not found in the index.\")\n\nprint_posting_list(\"samurai\") # ex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:05:22.207006Z","iopub.execute_input":"2025-02-01T19:05:22.207339Z","iopub.status.idle":"2025-02-01T19:05:22.216159Z","shell.execute_reply.started":"2025-02-01T19:05:22.207313Z","shell.execute_reply":"2025-02-01T19:05:22.215475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Boolean Retrieval","metadata":{}},{"cell_type":"markdown","source":"### AND Query","metadata":{}},{"cell_type":"code","source":"# AND Query: Retrieves documents that contain all specified terms\ndef and_query(*terms):\n    result = set(index.get(terms[0], {}).keys())\n    for term in terms[1:]:\n        result &= set(index.get(term, {}).keys())\n    return result\n    \nprint(\"AND Query (Chinese AND Samurai):\", and_query(\"chines\", \"samurai\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:20:50.669988Z","iopub.execute_input":"2025-02-01T20:20:50.670266Z","iopub.status.idle":"2025-02-01T20:20:50.676088Z","shell.execute_reply.started":"2025-02-01T20:20:50.670246Z","shell.execute_reply":"2025-02-01T20:20:50.675315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### OR Query","metadata":{}},{"cell_type":"code","source":"# OR Query: Retrieves documents that contain either of the terms\ndef or_query(*terms):\n    result = set()\n    for term in terms:\n        result |= set(index.get(term, {}).keys())\n    return result\n\nprint(\"OR Query (Chinese AND Samurai):\", or_query(\"chines\", \"samurai\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:20:38.409232Z","iopub.execute_input":"2025-02-01T20:20:38.409566Z","iopub.status.idle":"2025-02-01T20:20:38.418000Z","shell.execute_reply.started":"2025-02-01T20:20:38.409520Z","shell.execute_reply":"2025-02-01T20:20:38.417031Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### NOT query","metadata":{}},{"cell_type":"code","source":"# NOT Query: Retrieves documents that do not contain the specified term\ndef not_query(term):\n    all_docs = set(df[\"filename\"].tolist())\n    term_docs = set(index.get(term, {}).keys())  # Documents containing the term\n    return all_docs - term_docs # union - given\n\nprint(\"NOT Query (NOT Chinese):\", not_query(\"chines\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:20:21.603446Z","iopub.execute_input":"2025-02-01T20:20:21.603910Z","iopub.status.idle":"2025-02-01T20:20:21.684587Z","shell.execute_reply.started":"2025-02-01T20:20:21.603877Z","shell.execute_reply":"2025-02-01T20:20:21.683561Z"},"scrolled":true},"outputs":[],"execution_count":null}]}